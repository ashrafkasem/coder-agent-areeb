version: '3'

services:
  llm-agent:
    build:
      context: .
      args:
        # These build args can be overridden from command line:
        # docker-compose build --build-arg DEFAULT_MODEL_PATH=your/model
        DEFAULT_MODEL_PATH: ${DEFAULT_MODEL_PATH:-mistralai/Mistral-7B-Instruct-v0.2}
        DEFAULT_MAX_NEW_TOKENS: ${DEFAULT_MAX_NEW_TOKENS:-1024}
        PREDOWNLOAD_MODEL: ${PREDOWNLOAD_MODEL:-false}
    image: llm-agent
    ports:
      - "${PORT:-8000}:${PORT:-8000}"
    environment:
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
      - MODEL_PATH=${MODEL_PATH:-mistralai/Mistral-7B-Instruct-v0.2}
      - MAX_NEW_TOKENS=${MAX_NEW_TOKENS:-1024}
      # Optionally set a fixed API key for reproducibility (uncomment and set your value):
      # - INITIAL_API_KEY=your-fixed-api-key-here
      # Required for gated Hugging Face models:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - ./data:/app/data
      # Cache HuggingFace models between container restarts
      - huggingface-cache:/root/.cache/huggingface
      # Optionally persist the .api_key file outside the container:
      # - ./api_key_volume:/app/.api_key
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  huggingface-cache:
    # Named volume for HuggingFace cache to speed up model loading